{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blank_swift.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoMangiavacchi/Swift-TensorFlow-Sample-Notebooks/blob/master/FizzBuzz_Swift_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import Foundation\n",
        "import TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1iJ2NkG19Si",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "func bitEncode(_ n: UInt, bits: Int = 10) -> [Float] {\n",
        "    var array = [Float](repeating: 0, count: bits)\n",
        "    for i in 0..<bits {\n",
        "        array[bits - 1 - i] = Float(n >> i & 1)\n",
        "    }\n",
        "    return array\n",
        "}\n",
        "\n",
        "func labelEncode(_ n: UInt) -> [Float] {\n",
        "    guard n > 0 else { return [0, 0, 0, 1] }\n",
        "    var array: [Float] = [0, 0, 0, 0]\n",
        "\n",
        "    if n % 15 == 0 {\n",
        "        array[0] = 1\n",
        "    }\n",
        "    else if n % 3 == 0 {\n",
        "        array[1] = 1\n",
        "    }\n",
        "    else if n % 5 == 0 {\n",
        "        array[2] = 1\n",
        "    }\n",
        "    else {\n",
        "        array[3] = 1\n",
        "    }\n",
        "\n",
        "    return array\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2qwdXkr2viX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "let start = 1\n",
        "let end = 100\n",
        "var X = [[Float]]()\n",
        "var Y = [[Float]]()\n",
        "\n",
        "for i in start..<end {\n",
        "    X.append(bitEncode(UInt(i)))\n",
        "    Y.append(labelEncode(UInt(i)))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_e1hzXeb8J5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "struct FizzBuzz: Layer {\n",
        "    var l1: Dense<Float>\n",
        "    var l2: Dense<Float>\n",
        "    var l3: Dense<Float>\n",
        "\n",
        "\n",
        "    init(bits: Int = 1) {\n",
        "        l1 = Dense<Float>(inputSize: bits, outputSize: 64, activation: relu)\n",
        "        l2 = Dense<Float>(inputSize: 64, outputSize: 128, activation: relu)\n",
        "        l3 = Dense<Float>(inputSize: 128, outputSize: 4, activation: softmax) // {$0}\n",
        "    }\n",
        "    \n",
        "    @differentiable(wrt: (self, input))\n",
        "    func applied(to input: Tensor<Float>) -> Tensor<Float> {\n",
        "        let output1 = l1.applied(to: input)\n",
        "        let output2 = l2.applied(to: output1)\n",
        "        return l3.applied(to: output2)\n",
        "\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdGxV6K2VZ2X",
        "colab_type": "code",
        "outputId": "53b44d16-391f-4b3d-b4cd-d27622ad6a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "let x = Tensor<Float>(shape: [Int32(end - start), 10], scalars: Array(X.joined()))\n",
        "let y = Tensor<Float>(shape: [Int32(end - start), 4], scalars: Array(Y.joined()))\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorShape(dimensions: [99, 10])\r\n",
            "TensorShape(dimensions: [99, 4])\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2xoues9HoFec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "let batchSize: Int32 = 128\n",
        "let trainingIterations = Int32(end - start) / batchSize\n",
        "\n",
        "func minibatch<T>(_ x: Tensor<T>, batchIndex: Int32) -> Tensor<T> {\n",
        "  let start = batchIndex * batchSize\n",
        "  return x[start..<start+batchSize]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMTUjO5NoqoR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "174b2ddb-ac64-463c-fa6f-5cc3f8ccc6d0"
      },
      "cell_type": "code",
      "source": [
        "let optimizer = RMSProp<FizzBuzz, Float>(learningRate: 0.003)\n",
        "var model = FizzBuzz(bits: 10)\n",
        "\n",
        "\n",
        "for epoch in 1...10000 {\n",
        "    var totalLoss: Float = 0\n",
        "\n",
        "    for i in 0..<trainingIterations {\n",
        "        let xBatch = minibatch(x, batchIndex: i)\n",
        "        let yBatch = minibatch(y, batchIndex: i)\n",
        "\n",
        "        let gradients = gradient(at: model) { m -> Tensor<Float> in\n",
        "            let ≈∑ = m.applied(to: xBatch)\n",
        "            let batchLoss = softmaxCrossEntropy(logits: ≈∑, labels: yBatch)\n",
        "            totalLoss += batchLoss.scalarized()\n",
        "            return batchLoss\n",
        "        }\n",
        "        optimizer.update(&model.allDifferentiableVariables, along: gradients)\n",
        "    }\n",
        "\n",
        "\n",
        "    if epoch % 1000 == 0 {\n",
        "        print(\"Epoch \\(epoch) loss: \\(totalLoss)\")\n",
        "    }\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1000 loss: 0.0\r\n",
            "Epoch 2000 loss: 0.0\r\n",
            "Epoch 3000 loss: 0.0\r\n",
            "Epoch 4000 loss: 0.0\r\n",
            "Epoch 5000 loss: 0.0\r\n",
            "Epoch 6000 loss: 0.0\r\n",
            "Epoch 7000 loss: 0.0\r\n",
            "Epoch 8000 loss: 0.0\r\n",
            "Epoch 9000 loss: 0.0\r\n",
            "Epoch 10000 loss: 0.0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkEgM40GQGKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "// let optimizer = RMSProp<FizzBuzz, Float>(learningRate: 0.003)\n",
        "// var model = FizzBuzz(bits: 10)\n",
        "\n",
        "\n",
        "// for epoch in 1...1000 {\n",
        "//     let (cost, ùõÅmodel) = model.valueWithGradient { m -> Tensor<Float> in\n",
        "//         let ≈∑ = m.applied(to: x)\n",
        "//         return softmaxCrossEntropy(logits: ≈∑, labels: y)\n",
        "//     }\n",
        "//     optimizer.update(&model.allDifferentiableVariables, along: ùõÅmodel)\n",
        "  \n",
        "//     if epoch % 10 == 0 {\n",
        "//         print(\"Epoch: \\(epoch) Cost: \\(cost)\")\n",
        "//     }\n",
        "// }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8u2dYDdbC0OP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "var X_test = [[Float]]()\n",
        "var Y_true = [[Float]]()\n",
        "\n",
        "for i in 0..<100 {\n",
        "    X_test.append(bitEncode(UInt(i)))\n",
        "    Y_true.append(labelEncode(UInt(i)))\n",
        "}\n",
        "\n",
        "let xTest = Tensor<Float>(shape: [100, 10], scalars: Array(X_test.joined()))\n",
        "let yTest = model.applied(to: xTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JxsXeC37EqEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "4c981212-aedd-4739-8ad7-457b7f928369"
      },
      "cell_type": "code",
      "source": [
        "let yMax = yTest.argmax(squeezingAxis: 1).array\n",
        "let yArray = Array(yTest.array)\n",
        "let description = [\"FizzBuzz\", \"Fizz    \", \"Buzz    \", \"        \"]\n",
        "\n",
        "for i in 0..<100 {\n",
        "    print(i, description[Int(yMax[i].description)!], Y_true[i], yArray[i])\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.25, 0.25, 0.25, 0.25]\r\n",
            "1          [0.0, 0.0, 0.0, 1.0] [0.25509593, 0.23842248, 0.23629057, 0.270191]\r\n",
            "2          [0.0, 0.0, 0.0, 1.0] [0.24213266, 0.25685388, 0.22209178, 0.27892157]\r\n",
            "3          [0.0, 1.0, 0.0, 0.0] [0.2630091, 0.2397849, 0.21642081, 0.28078523]\r\n",
            "4 Buzz     [0.0, 0.0, 0.0, 1.0] [0.24197285, 0.245017, 0.25743186, 0.25557828]\r\n",
            "5          [0.0, 0.0, 1.0, 0.0] [0.25204873, 0.23316228, 0.2332494, 0.2815396]\r\n",
            "6          [0.0, 1.0, 0.0, 0.0] [0.24434917, 0.25553596, 0.19659962, 0.30351523]\r\n",
            "7          [0.0, 0.0, 0.0, 1.0] [0.25092223, 0.24028395, 0.19857872, 0.31021512]\r\n",
            "8 Buzz     [0.0, 0.0, 0.0, 1.0] [0.2530235, 0.24376775, 0.25791526, 0.24529356]\r\n",
            "9 FizzBuzz [0.0, 1.0, 0.0, 0.0] [0.26550248, 0.23208976, 0.24309665, 0.2593111]\r\n",
            "10          [0.0, 0.0, 1.0, 0.0] [0.24269448, 0.22957502, 0.24677868, 0.28095192]\r\n",
            "11          [0.0, 0.0, 0.0, 1.0] [0.27469966, 0.22789234, 0.21440391, 0.28300408]\r\n",
            "12 Buzz     [0.0, 1.0, 0.0, 0.0] [0.25066748, 0.22969352, 0.2857361, 0.23390289]\r\n",
            "13 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.2696855, 0.23377794, 0.24955174, 0.24698478]\r\n",
            "14          [0.0, 0.0, 0.0, 1.0] [0.25670585, 0.23186013, 0.23528552, 0.27614853]\r\n",
            "15          [1.0, 0.0, 0.0, 0.0] [0.25407898, 0.23169866, 0.21997558, 0.2942468]\r\n",
            "16 Buzz     [0.0, 0.0, 0.0, 1.0] [0.23178235, 0.24062572, 0.26550394, 0.26208794]\r\n",
            "17          [0.0, 0.0, 0.0, 1.0] [0.24143456, 0.24244246, 0.2518833, 0.2642396]\r\n",
            "18          [0.0, 1.0, 0.0, 0.0] [0.21215506, 0.23595685, 0.25650546, 0.29538262]\r\n",
            "19          [0.0, 0.0, 0.0, 1.0] [0.24695377, 0.23620892, 0.22953708, 0.28730023]\r\n",
            "20 Buzz     [0.0, 0.0, 1.0, 0.0] [0.22976097, 0.24041077, 0.266408, 0.26342022]\r\n",
            "21          [0.0, 1.0, 0.0, 0.0] [0.24812545, 0.2248559, 0.24596046, 0.2810582]\r\n",
            "22          [0.0, 0.0, 0.0, 1.0] [0.22641332, 0.2442961, 0.22647703, 0.30281353]\r\n",
            "23          [0.0, 0.0, 0.0, 1.0] [0.24058764, 0.2336558, 0.21313289, 0.31262362]\r\n",
            "24 Buzz     [0.0, 1.0, 0.0, 0.0] [0.22387762, 0.23747101, 0.29028502, 0.24836631]\r\n",
            "25          [0.0, 0.0, 1.0, 0.0] [0.25201577, 0.24246174, 0.2499343, 0.2555882]\r\n",
            "26          [0.0, 0.0, 0.0, 1.0] [0.21524505, 0.21924925, 0.2725408, 0.29296485]\r\n",
            "27          [0.0, 1.0, 0.0, 0.0] [0.25557476, 0.21626984, 0.2436133, 0.28454208]\r\n",
            "28 Buzz     [0.0, 0.0, 0.0, 1.0] [0.23127581, 0.23318858, 0.29407787, 0.24145769]\r\n",
            "29 Buzz     [0.0, 0.0, 0.0, 1.0] [0.24628498, 0.22479144, 0.27187553, 0.25704807]\r\n",
            "30          [1.0, 0.0, 0.0, 0.0] [0.22373627, 0.22455426, 0.2691817, 0.2825278]\r\n",
            "31          [0.0, 0.0, 0.0, 1.0] [0.24300222, 0.23289594, 0.23705173, 0.28705013]\r\n",
            "32          [0.0, 0.0, 0.0, 1.0] [0.24491052, 0.2496848, 0.2428454, 0.26255924]\r\n",
            "33 FizzBuzz [0.0, 1.0, 0.0, 0.0] [0.27477822, 0.22999135, 0.22747594, 0.26775447]\r\n",
            "34          [0.0, 0.0, 0.0, 1.0] [0.23853867, 0.24261992, 0.22150663, 0.2973348]\r\n",
            "35          [0.0, 0.0, 1.0, 0.0] [0.26250842, 0.23993716, 0.2048412, 0.29271317]\r\n",
            "36          [0.0, 1.0, 0.0, 0.0] [0.2473822, 0.23917364, 0.24591713, 0.267527]\r\n",
            "37          [0.0, 0.0, 0.0, 1.0] [0.26964352, 0.2219305, 0.22649181, 0.2819342]\r\n",
            "38          [0.0, 0.0, 0.0, 1.0] [0.2503734, 0.22905278, 0.19511405, 0.32545978]\r\n",
            "39          [0.0, 1.0, 0.0, 0.0] [0.27077943, 0.21376765, 0.19075981, 0.3246931]\r\n",
            "40          [0.0, 0.0, 1.0, 0.0] [0.25745612, 0.2313101, 0.25332612, 0.25790775]\r\n",
            "41 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.29106954, 0.22442017, 0.22069949, 0.2638108]\r\n",
            "42          [0.0, 1.0, 0.0, 0.0] [0.25228596, 0.22397971, 0.23398297, 0.2897514]\r\n",
            "43          [0.0, 0.0, 0.0, 1.0] [0.27574962, 0.22012666, 0.21227847, 0.2918452]\r\n",
            "44 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.26228294, 0.23651479, 0.258357, 0.24284531]\r\n",
            "45 FizzBuzz [1.0, 0.0, 0.0, 0.0] [0.2813952, 0.22615165, 0.23539168, 0.25706145]\r\n",
            "46          [0.0, 0.0, 0.0, 1.0] [0.26040834, 0.21677604, 0.22559646, 0.29721922]\r\n",
            "47          [0.0, 0.0, 0.0, 1.0] [0.28117663, 0.21154028, 0.20400162, 0.3032815]\r\n",
            "48          [0.0, 1.0, 0.0, 0.0] [0.24485862, 0.22057006, 0.26282004, 0.27175137]\r\n",
            "49 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.27834037, 0.22511071, 0.23090927, 0.26563963]\r\n",
            "50          [0.0, 0.0, 1.0, 0.0] [0.22254731, 0.220089, 0.24533725, 0.31202647]\r\n",
            "51          [0.0, 1.0, 0.0, 0.0] [0.2681146, 0.21272379, 0.2168406, 0.30232102]\r\n",
            "52          [0.0, 0.0, 0.0, 1.0] [0.23830166, 0.23252924, 0.25308323, 0.2760859]\r\n",
            "53          [0.0, 0.0, 0.0, 1.0] [0.26441485, 0.2037322, 0.24858814, 0.28326482]\r\n",
            "54          [0.0, 1.0, 0.0, 0.0] [0.2313966, 0.2197084, 0.22571269, 0.3231823]\r\n",
            "55          [0.0, 0.0, 1.0, 0.0] [0.2547105, 0.20837627, 0.21150737, 0.32540584]\r\n",
            "56          [0.0, 0.0, 0.0, 1.0] [0.24858846, 0.21459348, 0.26515254, 0.27166545]\r\n",
            "57 FizzBuzz [0.0, 1.0, 0.0, 0.0] [0.2851922, 0.22538055, 0.22101718, 0.26841012]\r\n",
            "58          [0.0, 0.0, 0.0, 1.0] [0.2243053, 0.20823427, 0.27459008, 0.29287037]\r\n",
            "59          [0.0, 0.0, 0.0, 1.0] [0.26019022, 0.20663273, 0.23652357, 0.29665348]\r\n",
            "60 Buzz     [1.0, 0.0, 0.0, 0.0] [0.24820457, 0.22740352, 0.27068114, 0.25371078]\r\n",
            "61 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.27466753, 0.22086348, 0.2410668, 0.2634022]\r\n",
            "62          [0.0, 0.0, 0.0, 1.0] [0.22483212, 0.20682602, 0.25700068, 0.3113411]\r\n",
            "63          [0.0, 1.0, 0.0, 0.0] [0.25553355, 0.2043137, 0.23015839, 0.30999428]\r\n",
            "64 Buzz     [0.0, 0.0, 0.0, 1.0] [0.24186885, 0.2377704, 0.26155055, 0.25881025]\r\n",
            "65          [0.0, 0.0, 1.0, 0.0] [0.26029983, 0.21917804, 0.25157702, 0.26894504]\r\n",
            "66          [0.0, 1.0, 0.0, 0.0] [0.23016968, 0.24568109, 0.24023733, 0.28391188]\r\n",
            "67          [0.0, 0.0, 0.0, 1.0] [0.25397032, 0.23232158, 0.22419348, 0.28951457]\r\n",
            "68 Buzz     [0.0, 0.0, 0.0, 1.0] [0.25198302, 0.22198927, 0.28309563, 0.24293207]\r\n",
            "69 FizzBuzz [0.0, 1.0, 0.0, 0.0] [0.26762006, 0.20770134, 0.25773278, 0.26694578]\r\n",
            "70          [0.0, 0.0, 1.0, 0.0] [0.23837426, 0.24211344, 0.23471355, 0.28479877]\r\n",
            "71          [0.0, 0.0, 0.0, 1.0] [0.24908939, 0.21571831, 0.22393931, 0.31125304]\r\n",
            "72 Buzz     [0.0, 1.0, 0.0, 0.0] [0.24009559, 0.21877488, 0.29927966, 0.24184993]\r\n",
            "73 Buzz     [0.0, 0.0, 0.0, 1.0] [0.2651735, 0.21352749, 0.266588, 0.25471112]\r\n",
            "74          [0.0, 0.0, 0.0, 1.0] [0.22730882, 0.22383858, 0.2713042, 0.27754837]\r\n",
            "75          [1.0, 0.0, 0.0, 0.0] [0.25326118, 0.21769844, 0.24114823, 0.2878921]\r\n",
            "76 Buzz     [0.0, 0.0, 0.0, 1.0] [0.2414539, 0.20624562, 0.31191546, 0.240385]\r\n",
            "77 Buzz     [0.0, 0.0, 0.0, 1.0] [0.27055255, 0.20207134, 0.28279456, 0.24458146]\r\n",
            "78 Buzz     [0.0, 1.0, 0.0, 0.0] [0.23902796, 0.21380891, 0.27760163, 0.2695615]\r\n",
            "79          [0.0, 0.0, 0.0, 1.0] [0.25806743, 0.21143357, 0.25240675, 0.27809224]\r\n",
            "80 Buzz     [0.0, 0.0, 1.0, 0.0] [0.2272809, 0.24117506, 0.28464594, 0.24689814]\r\n",
            "81 Buzz     [0.0, 1.0, 0.0, 0.0] [0.24482302, 0.22496581, 0.26912957, 0.26108155]\r\n",
            "82 Buzz     [0.0, 0.0, 0.0, 1.0] [0.20501111, 0.2329957, 0.29036868, 0.2716245]\r\n",
            "83          [0.0, 0.0, 0.0, 1.0] [0.24646032, 0.21823598, 0.2567197, 0.278584]\r\n",
            "84 Buzz     [0.0, 1.0, 0.0, 0.0] [0.23912165, 0.22201471, 0.2872288, 0.25163487]\r\n",
            "85 Buzz     [0.0, 0.0, 1.0, 0.0] [0.24683852, 0.20707247, 0.28010878, 0.2659802]\r\n",
            "86          [0.0, 0.0, 0.0, 1.0] [0.2249895, 0.2250714, 0.25925744, 0.29068166]\r\n",
            "87          [0.0, 1.0, 0.0, 0.0] [0.244545, 0.20644686, 0.24418533, 0.30482277]\r\n",
            "88 Buzz     [0.0, 0.0, 0.0, 1.0] [0.21404527, 0.2247802, 0.32393014, 0.23724434]\r\n",
            "89 Buzz     [0.0, 0.0, 0.0, 1.0] [0.23941693, 0.21569374, 0.2926281, 0.25226122]\r\n",
            "90 Buzz     [1.0, 0.0, 0.0, 0.0] [0.20416987, 0.21675101, 0.31098306, 0.26809603]\r\n",
            "91          [0.0, 0.0, 0.0, 1.0] [0.24119513, 0.20750676, 0.271702, 0.27959618]\r\n",
            "92 Buzz     [0.0, 0.0, 0.0, 1.0] [0.22432064, 0.20292181, 0.33454025, 0.23821731]\r\n",
            "93 Buzz     [0.0, 1.0, 0.0, 0.0] [0.24497028, 0.20022643, 0.30568862, 0.24911469]\r\n",
            "94 Buzz     [0.0, 0.0, 0.0, 1.0] [0.21634385, 0.20066255, 0.31301546, 0.2699781]\r\n",
            "95          [0.0, 0.0, 1.0, 0.0] [0.24382497, 0.20134847, 0.2759792, 0.27884737]\r\n",
            "96          [0.0, 1.0, 0.0, 0.0] [0.24017657, 0.23790462, 0.25384966, 0.26806912]\r\n",
            "97 FizzBuzz [0.0, 0.0, 0.0, 1.0] [0.28324154, 0.204847, 0.22972801, 0.28218344]\r\n",
            "98          [0.0, 0.0, 0.0, 1.0] [0.2308482, 0.24179748, 0.23759538, 0.28975895]\r\n",
            "99          [0.0, 1.0, 0.0, 0.0] [0.26944104, 0.2244449, 0.21398544, 0.29212856]\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}