{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blank_swift.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoMangiavacchi/Swift-TensorFlow-Sample-Notebooks/blob/master/XOR_Swift_TensorFlow_2-2-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "n8HLsFTOM_b2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **XOR Classifier**\n",
        "\n",
        "Classic Trivial Model for the XOR (^) Bit Mutual Exclusive Operation\n",
        "\n",
        "\n",
        "```\n",
        "0 ^ 0 = 0\n",
        "1 ^ 0 = 1\n",
        "0 ^ 1 = 1\n",
        "1 ^ 1 = 0\n",
        "```\n",
        "\n",
        "**Swift Classic 2:2:1 Architecture**\n",
        "\n",
        "Swift TensorFlow implementation with 1 Hidden Layer with only two neurons and sigmoid activation function. This architecure force the model to mimic the following XOR formula: \n",
        "\n",
        "```\n",
        "XOR(x1, x2) = !((x1 & x2) | (!x1 & !x2))\n",
        "```\n",
        "\n",
        "This model is pretty slow to converge and it needs lots of epoch interactions to converge.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import TensorFlow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_e1hzXeb8J5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "struct XOR: Layer {\n",
        "    var l1, l2: Dense<Float>\n",
        "    init() {\n",
        "        l1 = Dense<Float>(\n",
        "            inputSize: 2,\n",
        "            outputSize: 2,\n",
        "            activation: sigmoid,\n",
        "            seed: (0xfffffff, 0xfeeff)\n",
        "        )\n",
        "        l2 = Dense<Float>(\n",
        "            inputSize: 2,\n",
        "            outputSize: 1,\n",
        "            activation: sigmoid,\n",
        "            seed: (0xfeffeffe, 0xfffe)\n",
        "        )\n",
        "    }\n",
        "  \n",
        "    @differentiable\n",
        "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
        "        let h1 = l1.applied(to: input, in: context)\n",
        "        return l2.applied(to: h1, in: context)\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JdGxV6K2VZ2X",
        "colab_type": "code",
        "outputId": "41d4abe7-5e4f-4d1e-f205-b6d719119be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17017
        }
      },
      "cell_type": "code",
      "source": [
        "var model = XOR()\n",
        "let optimizer = SGD(for: model, learningRate: 0.02, scalarType: Float.self)\n",
        "\n",
        "let x: Tensor<Float> = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "let y: Tensor<Float> = [[0], [1], [1], [0]]\n",
        "\n",
        "let trainingContext = Context(learningPhase: .training)\n",
        "for epoch in 1...100000 {\n",
        "    let ùõÅmodel = model.gradient { m -> Tensor<Float> in\n",
        "        let ≈∑ = m.applied(to: x, in: trainingContext)\n",
        "        let loss = meanSquaredError(predicted: ≈∑, expected: y)\n",
        "        //if epoch % 100 == 0 {\n",
        "          print(\"Epoch: \\(epoch) Loss: \\(loss)\")\n",
        "        //}\n",
        "\n",
        "        return loss\n",
        "    }\n",
        "    optimizer.update(&model.allDifferentiableVariables, along: ùõÅmodel)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Loss: 0.4869756\r\n",
            "Epoch: 2 Loss: 0.45177865\r\n",
            "Epoch: 3 Loss: 0.4216553\r\n",
            "Epoch: 4 Loss: 0.39586228\r\n",
            "Epoch: 5 Loss: 0.37377304\r\n",
            "Epoch: 6 Loss: 0.35485643\r\n",
            "Epoch: 7 Loss: 0.33865982\r\n",
            "Epoch: 8 Loss: 0.32479632\r\n",
            "Epoch: 9 Loss: 0.31293443\r\n",
            "Epoch: 10 Loss: 0.3027894\r\n",
            "Epoch: 11 Loss: 0.29411638\r\n",
            "Epoch: 12 Loss: 0.28670478\r\n",
            "Epoch: 13 Loss: 0.2803734\r\n",
            "Epoch: 14 Loss: 0.27496624\r\n",
            "Epoch: 15 Loss: 0.2703492\r\n",
            "Epoch: 16 Loss: 0.26640695\r\n",
            "Epoch: 17 Loss: 0.26304048\r\n",
            "Epoch: 18 Loss: 0.26016486\r\n",
            "Epoch: 19 Loss: 0.25770712\r\n",
            "Epoch: 20 Loss: 0.25560495\r\n",
            "Epoch: 21 Loss: 0.25380498\r\n",
            "Epoch: 22 Loss: 0.25226158\r\n",
            "Epoch: 23 Loss: 0.2509358\r\n",
            "Epoch: 24 Loss: 0.24979445\r\n",
            "Epoch: 25 Loss: 0.24880926\r\n",
            "Epoch: 26 Loss: 0.24795616\r\n",
            "Epoch: 27 Loss: 0.24721467\r\n",
            "Epoch: 28 Loss: 0.24656741\r\n",
            "Epoch: 29 Loss: 0.2459996\r\n",
            "Epoch: 30 Loss: 0.24549878\r\n",
            "Epoch: 31 Loss: 0.24505429\r\n",
            "Epoch: 32 Loss: 0.24465714\r\n",
            "Epoch: 33 Loss: 0.24429977\r\n",
            "Epoch: 34 Loss: 0.24397567\r\n",
            "Epoch: 35 Loss: 0.2436795\r\n",
            "Epoch: 36 Loss: 0.2434066\r\n",
            "Epoch: 37 Loss: 0.24315307\r\n",
            "Epoch: 38 Loss: 0.24291566\r\n",
            "Epoch: 39 Loss: 0.24269158\r\n",
            "Epoch: 40 Loss: 0.24247852\r\n",
            "Epoch: 41 Loss: 0.24227446\r\n",
            "Epoch: 42 Loss: 0.24207777\n",
            "Epoch: 43 Loss: 0.24188702\n",
            "Epoch: 44 Loss: 0.24170104\n",
            "Epoch: 45 Loss: 0.2415188\n",
            "Epoch: 46 Loss: 0.24133946\n",
            "Epoch: 47 Loss: 0.24116233\n",
            "Epoch: 48 Loss: 0.2409868\n",
            "Epoch: 49 Loss: 0.24081227\n",
            "Epoch: 50 Loss: 0.24063843\n",
            "Epoch: 51 Loss: 0.24046487\n",
            "Epoch: 52 Loss: 0.24029127\n",
            "Epoch: 53 Loss: 0.24011734\n",
            "Epoch: 54 Loss: 0.2399429\n",
            "Epoch: 55 Loss: 0.23976767\n",
            "Epoch: 56 Loss: 0.2395916\n",
            "Epoch: 57 Loss: 0.23941448\n",
            "Epoch: 58 Loss: 0.23923622\n",
            "Epoch: 59 Loss: 0.23905666\n",
            "Epoch: 60 Loss: 0.23887579\n",
            "Epoch: 61 Loss: 0.23869345\n",
            "Epoch: 62 Loss: 0.23850963\n",
            "Epoch: 63 Loss: 0.23832424\n",
            "Epoch: 64 Loss: 0.23813723\n",
            "Epoch: 65 Loss: 0.23794852\n",
            "Epoch: 66 Loss: 0.23775813\n",
            "Epoch: 67 Loss: 0.23756596\n",
            "Epoch: 68 Loss: 0.23737209\n",
            "Epoch: 69 Loss: 0.2371763\n",
            "Epoch: 70 Loss: 0.23697871\n",
            "Epoch: 71 Loss: 0.23677921\n",
            "Epoch: 72 Loss: 0.23657784\n",
            "Epoch: 73 Loss: 0.23637454\n",
            "Epoch: 74 Loss: 0.23616931\n",
            "Epoch: 75 Loss: 0.23596206\n",
            "Epoch: 76 Loss: 0.23575285\n",
            "Epoch: 77 Loss: 0.23554161\n",
            "Epoch: 78 Loss: 0.23532835\n",
            "Epoch: 79 Loss: 0.23511302\n",
            "Epoch: 80 Loss: 0.23489565\n",
            "Epoch: 81 Loss: 0.23467618\n",
            "Epoch: 82 Loss: 0.23445457\n",
            "Epoch: 83 Loss: 0.2342309\n",
            "Epoch: 84 Loss: 0.23400506\n",
            "Epoch: 85 Loss: 0.23377708\n",
            "Epoch: 86 Loss: 0.23354691\n",
            "Epoch: 87 Loss: 0.23331459\n",
            "Epoch: 88 Loss: 0.23308006\n",
            "Epoch: 89 Loss: 0.23284328\n",
            "Epoch: 90 Loss: 0.23260432\n",
            "Epoch: 91 Loss: 0.23236309\n",
            "Epoch: 92 Loss: 0.23211963\n",
            "Epoch: 93 Loss: 0.23187387\n",
            "Epoch: 94 Loss: 0.23162585\n",
            "Epoch: 95 Loss: 0.23137555\n",
            "Epoch: 96 Loss: 0.23112293\n",
            "Epoch: 97 Loss: 0.23086795\n",
            "Epoch: 98 Loss: 0.23061067\n",
            "Epoch: 99 Loss: 0.23035103\n",
            "Epoch: 100 Loss: 0.23008907\n",
            "Epoch: 101 Loss: 0.22982472\n",
            "Epoch: 102 Loss: 0.22955798\n",
            "Epoch: 103 Loss: 0.22928883\n",
            "Epoch: 104 Loss: 0.22901729\n",
            "Epoch: 105 Loss: 0.22874337\n",
            "Epoch: 106 Loss: 0.22846703\n",
            "Epoch: 107 Loss: 0.22818823\n",
            "Epoch: 108 Loss: 0.22790697\n",
            "Epoch: 109 Loss: 0.22762328\n",
            "Epoch: 110 Loss: 0.22733715\n",
            "Epoch: 111 Loss: 0.22704852\n",
            "Epoch: 112 Loss: 0.2267574\n",
            "Epoch: 113 Loss: 0.22646385\n",
            "Epoch: 114 Loss: 0.22616777\n",
            "Epoch: 115 Loss: 0.2258692\n",
            "Epoch: 116 Loss: 0.2255681\n",
            "Epoch: 117 Loss: 0.22526453\n",
            "Epoch: 118 Loss: 0.22495842\n",
            "Epoch: 119 Loss: 0.22464976\n",
            "Epoch: 120 Loss: 0.22433859\n",
            "Epoch: 121 Loss: 0.22402489\n",
            "Epoch: 122 Loss: 0.22370861\n",
            "Epoch: 123 Loss: 0.22338983\n",
            "Epoch: 124 Loss: 0.22306848\n",
            "Epoch: 125 Loss: 0.22274455\n",
            "Epoch: 126 Loss: 0.22241808\n",
            "Epoch: 127 Loss: 0.22208907\n",
            "Epoch: 128 Loss: 0.22175747\n",
            "Epoch: 129 Loss: 0.22142328\n",
            "Epoch: 130 Loss: 0.22108653\n",
            "Epoch: 131 Loss: 0.22074726\n",
            "Epoch: 132 Loss: 0.22040537\n",
            "Epoch: 133 Loss: 0.22006091\n",
            "Epoch: 134 Loss: 0.21971387\n",
            "Epoch: 135 Loss: 0.21936424\n",
            "Epoch: 136 Loss: 0.21901208\n",
            "Epoch: 137 Loss: 0.21865733\n",
            "Epoch: 138 Loss: 0.2183\n",
            "Epoch: 139 Loss: 0.21794008\n",
            "Epoch: 140 Loss: 0.2175776\n",
            "Epoch: 141 Loss: 0.21721256\n",
            "Epoch: 142 Loss: 0.21684493\n",
            "Epoch: 143 Loss: 0.21647476\n",
            "Epoch: 144 Loss: 0.21610202\n",
            "Epoch: 145 Loss: 0.21572672\n",
            "Epoch: 146 Loss: 0.21534887\n",
            "Epoch: 147 Loss: 0.21496847\n",
            "Epoch: 148 Loss: 0.21458551\n",
            "Epoch: 149 Loss: 0.21420002\n",
            "Epoch: 150 Loss: 0.21381201\n",
            "Epoch: 151 Loss: 0.21342143\n",
            "Epoch: 152 Loss: 0.2130284\n",
            "Epoch: 153 Loss: 0.2126328\n",
            "Epoch: 154 Loss: 0.21223468\n",
            "Epoch: 155 Loss: 0.21187481\n",
            "Epoch: 156 Loss: 0.21164083\n",
            "Epoch: 157 Loss: 0.21123445\n",
            "Epoch: 158 Loss: 0.21082562\n",
            "Epoch: 159 Loss: 0.2104143\n",
            "Epoch: 160 Loss: 0.21012977\n",
            "Epoch: 161 Loss: 0.20980105\n",
            "Epoch: 162 Loss: 0.20938122\n",
            "Epoch: 163 Loss: 0.20895901\n",
            "Epoch: 164 Loss: 0.20853442\n",
            "Epoch: 165 Loss: 0.20831928\n",
            "Epoch: 166 Loss: 0.20790136\n",
            "Epoch: 167 Loss: 0.20746809\n",
            "Epoch: 168 Loss: 0.20703253\n",
            "Epoch: 169 Loss: 0.20665252\n",
            "Epoch: 170 Loss: 0.20638263\n",
            "Epoch: 171 Loss: 0.20593834\n",
            "Epoch: 172 Loss: 0.20549184\n",
            "Epoch: 173 Loss: 0.2050431\n",
            "Epoch: 174 Loss: 0.20471625\n",
            "Epoch: 175 Loss: 0.20437308\n",
            "Epoch: 176 Loss: 0.20391554\n",
            "Epoch: 177 Loss: 0.20345588\n",
            "Epoch: 178 Loss: 0.2029941\n",
            "Epoch: 179 Loss: 0.20270516\n",
            "Epoch: 180 Loss: 0.20251516\n",
            "Epoch: 181 Loss: 0.20188424\n",
            "Epoch: 182 Loss: 0.20148605\n",
            "Epoch: 183 Loss: 0.20159063\n",
            "Epoch: 184 Loss: 0.20089838\n",
            "Epoch: 185 Loss: 0.20066124\n",
            "Epoch: 186 Loss: 0.20039707\n",
            "Epoch: 187 Loss: 0.19970003\n",
            "Epoch: 188 Loss: 0.19981918\n",
            "Epoch: 189 Loss: 0.19920379\n",
            "Epoch: 190 Loss: 0.19875592\n",
            "Epoch: 191 Loss: 0.19870275\n",
            "Epoch: 192 Loss: 0.19799513\n",
            "Epoch: 193 Loss: 0.19788855\n",
            "Epoch: 194 Loss: 0.19749942\n",
            "Epoch: 195 Loss: 0.19683346\n",
            "Epoch: 196 Loss: 0.1965582\n",
            "Epoch: 197 Loss: 0.19628832\n",
            "Epoch: 198 Loss: 0.19588324\n",
            "Epoch: 199 Loss: 0.1957925\n",
            "Epoch: 200 Loss: 0.1950706\n",
            "Epoch: 201 Loss: 0.19497125\n",
            "Epoch: 202 Loss: 0.19458008\n",
            "Epoch: 203 Loss: 0.1939272\n",
            "Epoch: 204 Loss: 0.19354618\n",
            "Epoch: 205 Loss: 0.19336005\n",
            "Epoch: 206 Loss: 0.19285637\n",
            "Epoch: 207 Loss: 0.19286864\n",
            "Epoch: 208 Loss: 0.1921337\n",
            "Epoch: 209 Loss: 0.19189815\n",
            "Epoch: 210 Loss: 0.19164777\n",
            "Epoch: 211 Loss: 0.19096473\n",
            "Epoch: 212 Loss: 0.19048132\n",
            "Epoch: 213 Loss: 0.19063652\n",
            "Epoch: 214 Loss: 0.19000326\n",
            "Epoch: 215 Loss: 0.18936765\n",
            "Epoch: 216 Loss: 0.1895175\n",
            "Epoch: 217 Loss: 0.18876983\n",
            "Epoch: 218 Loss: 0.18835399\n",
            "Epoch: 219 Loss: 0.18828967\n",
            "Epoch: 220 Loss: 0.18753985\n",
            "Epoch: 221 Loss: 0.1873169\n",
            "Epoch: 222 Loss: 0.18706481\n",
            "Epoch: 223 Loss: 0.18631285\n",
            "Epoch: 224 Loss: 0.18625629\n",
            "Epoch: 225 Loss: 0.18584253\n",
            "Epoch: 226 Loss: 0.18508855\n",
            "Epoch: 227 Loss: 0.18517211\n",
            "Epoch: 228 Loss: 0.18462256\n",
            "Epoch: 229 Loss: 0.18391457\n",
            "Epoch: 230 Loss: 0.18352804\n",
            "Epoch: 231 Loss: 0.18339404\n",
            "Epoch: 232 Loss: 0.18273237\n",
            "Epoch: 233 Loss: 0.18235412\n",
            "Epoch: 234 Loss: 0.1821685\n",
            "Epoch: 235 Loss: 0.18154453\n",
            "Epoch: 236 Loss: 0.18116704\n",
            "Epoch: 237 Loss: 0.18127853\n",
            "Epoch: 238 Loss: 0.18051273\n",
            "Epoch: 239 Loss: 0.18000025\n",
            "Epoch: 240 Loss: 0.18005165\n",
            "Epoch: 241 Loss: 0.17928487\n",
            "Epoch: 242 Loss: 0.17880711\n",
            "Epoch: 243 Loss: 0.1788273\n",
            "Epoch: 244 Loss: 0.17805953\n",
            "Epoch: 245 Loss: 0.17758773\n",
            "Epoch: 246 Loss: 0.17760499\n",
            "Epoch: 247 Loss: 0.1768362\n",
            "Epoch: 248 Loss: 0.1763422\n",
            "Epoch: 249 Loss: 0.17638412\n",
            "Epoch: 250 Loss: 0.1756749\n",
            "Epoch: 251 Loss: 0.17513773\n",
            "Epoch: 252 Loss: 0.17527786\n",
            "Epoch: 253 Loss: 0.17453113\n",
            "Epoch: 254 Loss: 0.17394964\n",
            "Epoch: 255 Loss: 0.1741558\n",
            "Epoch: 256 Loss: 0.17340279\n",
            "Epoch: 257 Loss: 0.17275324\n",
            "Epoch: 258 Loss: 0.17301634\n",
            "Epoch: 259 Loss: 0.17231752\n",
            "Epoch: 260 Loss: 0.171585\n",
            "Epoch: 261 Loss: 0.17130303\n",
            "Epoch: 262 Loss: 0.17119437\n",
            "Epoch: 263 Loss: 0.17048377\n",
            "Epoch: 264 Loss: 0.17061687\n",
            "Epoch: 265 Loss: 0.1701228\n",
            "Epoch: 266 Loss: 0.16940527\n",
            "Epoch: 267 Loss: 0.16932207\n",
            "Epoch: 268 Loss: 0.16906542\n",
            "Epoch: 269 Loss: 0.16830865\n",
            "Epoch: 270 Loss: 0.16805233\n",
            "Epoch: 271 Loss: 0.16796196\n",
            "Epoch: 272 Loss: 0.16728011\n",
            "Epoch: 273 Loss: 0.16670832\n",
            "Epoch: 274 Loss: 0.16693082\n",
            "Epoch: 275 Loss: 0.16619036\n",
            "Epoch: 276 Loss: 0.1655617\n",
            "Epoch: 277 Loss: 0.16512893\n",
            "Epoch: 278 Loss: 0.1653456\n",
            "Epoch: 279 Loss: 0.16481829\n",
            "Epoch: 280 Loss: 0.16408741\n",
            "Epoch: 281 Loss: 0.16397361\n",
            "Epoch: 282 Loss: 0.16375944\n",
            "Epoch: 283 Loss: 0.1630937\n",
            "Epoch: 284 Loss: 0.1625491\n",
            "Epoch: 285 Loss: 0.16242738\n",
            "Epoch: 286 Loss: 0.16177644\n",
            "Epoch: 287 Loss: 0.16137803\n",
            "Epoch: 288 Loss: 0.16150483\n",
            "Epoch: 289 Loss: 0.16106786\n",
            "Epoch: 290 Loss: 0.16035624\n",
            "Epoch: 291 Loss: 0.16006087\n",
            "Epoch: 292 Loss: 0.16006282\n",
            "Epoch: 293 Loss: 0.15935579\n",
            "Epoch: 294 Loss: 0.15868358\n",
            "Epoch: 295 Loss: 0.15896377\n",
            "Epoch: 296 Loss: 0.15840617\n",
            "Epoch: 297 Loss: 0.15767863\n",
            "Epoch: 298 Loss: 0.15747884\n",
            "Epoch: 299 Loss: 0.15738039\n",
            "Epoch: 300 Loss: 0.15675335\n",
            "Epoch: 301 Loss: 0.15601662\n",
            "Epoch: 302 Loss: 0.15635209\n",
            "Epoch: 303 Loss: 0.15575007\n",
            "Epoch: 304 Loss: 0.15511274\n",
            "Epoch: 305 Loss: 0.1547273\n",
            "Epoch: 306 Loss: 0.15482438\n",
            "Epoch: 307 Loss: 0.15412411\n",
            "Epoch: 308 Loss: 0.15347129\n",
            "Epoch: 309 Loss: 0.15354969\n",
            "Epoch: 310 Loss: 0.15319309\n",
            "Epoch: 311 Loss: 0.1525288\n",
            "Epoch: 312 Loss: 0.15193823\n",
            "Epoch: 313 Loss: 0.15191092\n",
            "Epoch: 314 Loss: 0.15126672\n",
            "Epoch: 315 Loss: 0.15094563\n",
            "Epoch: 316 Loss: 0.15064517\n",
            "Epoch: 317 Loss: 0.1506866\n",
            "Epoch: 318 Loss: 0.14997207\n",
            "Epoch: 319 Loss: 0.14937343\n",
            "Epoch: 320 Loss: 0.14934444\n",
            "Epoch: 321 Loss: 0.14912203\n",
            "Epoch: 322 Loss: 0.1483908\n",
            "Epoch: 323 Loss: 0.14782348\n",
            "Epoch: 324 Loss: 0.148002\n",
            "Epoch: 325 Loss: 0.14756344\n",
            "Epoch: 326 Loss: 0.14685076\n",
            "Epoch: 327 Loss: 0.14633769\n",
            "Epoch: 328 Loss: 0.14624849\n",
            "Epoch: 329 Loss: 0.14565715\n",
            "Epoch: 330 Loss: 0.14531456\n",
            "Epoch: 331 Loss: 0.14489946\n",
            "Epoch: 332 Loss: 0.14506091\n",
            "Epoch: 333 Loss: 0.14444292\n",
            "Epoch: 334 Loss: 0.14377885\n",
            "Epoch: 335 Loss: 0.14347702\n",
            "Epoch: 336 Loss: 0.14352801\n",
            "Epoch: 337 Loss: 0.14289977\n",
            "Epoch: 338 Loss: 0.14225662\n",
            "Epoch: 339 Loss: 0.14201035\n",
            "Epoch: 340 Loss: 0.14200675\n",
            "Epoch: 341 Loss: 0.1413687\n",
            "Epoch: 342 Loss: 0.14074415\n",
            "Epoch: 343 Loss: 0.14050415\n",
            "Epoch: 344 Loss: 0.14049365\n",
            "Epoch: 345 Loss: 0.13984713\n",
            "Epoch: 346 Loss: 0.13923827\n",
            "Epoch: 347 Loss: 0.13896233\n",
            "Epoch: 348 Loss: 0.13898592\n",
            "Epoch: 349 Loss: 0.13833283\n",
            "Epoch: 350 Loss: 0.13773641\n",
            "Epoch: 351 Loss: 0.13738824\n",
            "Epoch: 352 Loss: 0.13748111\n",
            "Epoch: 353 Loss: 0.13682388\n",
            "Epoch: 354 Loss: 0.13623634\n",
            "Epoch: 355 Loss: 0.13581453\n",
            "Epoch: 356 Loss: 0.13564296\n",
            "Epoch: 357 Loss: 0.13499682\n",
            "Epoch: 358 Loss: 0.13487339\n",
            "Epoch: 359 Loss: 0.13473034\n",
            "Epoch: 360 Loss: 0.13414617\n",
            "Epoch: 361 Loss: 0.13350083\n",
            "Epoch: 362 Loss: 0.13321435\n",
            "Epoch: 363 Loss: 0.1332307\n",
            "Epoch: 364 Loss: 0.13264512\n",
            "Epoch: 365 Loss: 0.13200468\n",
            "Epoch: 366 Loss: 0.13155931\n",
            "Epoch: 367 Loss: 0.13112845\n",
            "Epoch: 368 Loss: 0.13104825\n",
            "Epoch: 369 Loss: 0.13063273\n",
            "Epoch: 370 Loss: 0.1298617\n",
            "Epoch: 371 Loss: 0.12905487\n",
            "Epoch: 372 Loss: 0.12860845\n",
            "Epoch: 373 Loss: 0.12809107\n",
            "Epoch: 374 Loss: 0.12733236\n",
            "Epoch: 375 Loss: 0.1269567\n",
            "Epoch: 376 Loss: 0.12668149\n",
            "Epoch: 377 Loss: 0.12592319\n",
            "Epoch: 378 Loss: 0.12514263\n",
            "Epoch: 379 Loss: 0.12447094\n",
            "Epoch: 380 Loss: 0.12394764\n",
            "Epoch: 381 Loss: 0.123649195\n",
            "Epoch: 382 Loss: 0.12299223\n",
            "Epoch: 383 Loss: 0.122338295\n",
            "Epoch: 384 Loss: 0.12184451\n",
            "Epoch: 385 Loss: 0.12169686\n",
            "Epoch: 386 Loss: 0.120853946\n",
            "Epoch: 387 Loss: 0.12022829\n",
            "Epoch: 388 Loss: 0.119429156\n",
            "Epoch: 389 Loss: 0.11936845\n",
            "Epoch: 390 Loss: 0.11878412\n",
            "Epoch: 391 Loss: 0.11809237\n",
            "Epoch: 392 Loss: 0.11737124\n",
            "Epoch: 393 Loss: 0.11670768\n",
            "Epoch: 394 Loss: 0.11644195\n",
            "Epoch: 395 Loss: 0.11568374\n",
            "Epoch: 396 Loss: 0.11519283\n",
            "Epoch: 397 Loss: 0.11453432\n",
            "Epoch: 398 Loss: 0.11407123\n",
            "Epoch: 399 Loss: 0.11359837\n",
            "Epoch: 400 Loss: 0.112995096\n",
            "Epoch: 401 Loss: 0.11232631\n",
            "Epoch: 402 Loss: 0.11184871\n",
            "Epoch: 403 Loss: 0.11125159\n",
            "Epoch: 404 Loss: 0.11086506\n",
            "Epoch: 405 Loss: 0.11021451\n",
            "Epoch: 406 Loss: 0.10947479\n",
            "Epoch: 407 Loss: 0.10912471\n",
            "Epoch: 408 Loss: 0.10839247\n",
            "Epoch: 409 Loss: 0.10816813\n",
            "Epoch: 410 Loss: 0.10740039\n",
            "Epoch: 411 Loss: 0.106776446\n",
            "Epoch: 412 Loss: 0.10638883\n",
            "Epoch: 413 Loss: 0.106012784\n",
            "Epoch: 414 Loss: 0.1053648\n",
            "Epoch: 415 Loss: 0.10461099\n",
            "Epoch: 416 Loss: 0.10404817\n",
            "Epoch: 417 Loss: 0.10333864\n",
            "Epoch: 418 Loss: 0.10305758\n",
            "Epoch: 419 Loss: 0.102323964\n",
            "Epoch: 420 Loss: 0.101777375\n",
            "Epoch: 421 Loss: 0.10124428\n",
            "Epoch: 422 Loss: 0.10053398\n",
            "Epoch: 423 Loss: 0.10020268\n",
            "Epoch: 424 Loss: 0.09973596\n",
            "Epoch: 425 Loss: 0.09917893\n",
            "Epoch: 426 Loss: 0.09842715\n",
            "Epoch: 427 Loss: 0.097800344\n",
            "Epoch: 428 Loss: 0.09718631\n",
            "Epoch: 429 Loss: 0.0967516\n",
            "Epoch: 430 Loss: 0.09616238\n",
            "Epoch: 431 Loss: 0.0954059\n",
            "Epoch: 432 Loss: 0.09502151\n",
            "Epoch: 433 Loss: 0.09462781\n",
            "Epoch: 434 Loss: 0.093988016\n",
            "Epoch: 435 Loss: 0.09333223\n",
            "Epoch: 436 Loss: 0.09261487\n",
            "Epoch: 437 Loss: 0.09205678\n",
            "Epoch: 438 Loss: 0.0915134\n",
            "Epoch: 439 Loss: 0.09121826\n",
            "Epoch: 440 Loss: 0.090499245\n",
            "Epoch: 441 Loss: 0.08983587\n",
            "Epoch: 442 Loss: 0.08922296\n",
            "Epoch: 443 Loss: 0.08847821\n",
            "Epoch: 444 Loss: 0.08813049\n",
            "Epoch: 445 Loss: 0.08766168\n",
            "Epoch: 446 Loss: 0.087040834\n",
            "Epoch: 447 Loss: 0.08638403\n",
            "Epoch: 448 Loss: 0.085674845\n",
            "Epoch: 449 Loss: 0.08512364\n",
            "Epoch: 450 Loss: 0.08456178\n",
            "Epoch: 451 Loss: 0.08405796\n",
            "Epoch: 452 Loss: 0.083373584\n",
            "Epoch: 453 Loss: 0.08270471\n",
            "Epoch: 454 Loss: 0.08237457\n",
            "Epoch: 455 Loss: 0.08179983\n",
            "Epoch: 456 Loss: 0.0812429\n",
            "Epoch: 457 Loss: 0.080537006\n",
            "Epoch: 458 Loss: 0.07988238\n",
            "Epoch: 459 Loss: 0.07929055\n",
            "Epoch: 460 Loss: 0.078662366\n",
            "Epoch: 461 Loss: 0.078417\n",
            "Epoch: 462 Loss: 0.07770157\n",
            "Epoch: 463 Loss: 0.077050775\n",
            "Epoch: 464 Loss: 0.076455966\n",
            "Epoch: 465 Loss: 0.07573559\n",
            "Epoch: 466 Loss: 0.07526453\n",
            "Epoch: 467 Loss: 0.07471057\n",
            "Epoch: 468 Loss: 0.07406388\n",
            "Epoch: 469 Loss: 0.07348175\n",
            "Epoch: 470 Loss: 0.07286986\n",
            "Epoch: 471 Loss: 0.07258164\n",
            "Epoch: 472 Loss: 0.07189395\n",
            "Epoch: 473 Loss: 0.07123174\n",
            "Epoch: 474 Loss: 0.07066859\n",
            "Epoch: 475 Loss: 0.069964044\n",
            "Epoch: 476 Loss: 0.06939849\n",
            "Epoch: 477 Loss: 0.068939924\n",
            "Epoch: 478 Loss: 0.06826371\n",
            "Epoch: 479 Loss: 0.067733474\n",
            "Epoch: 480 Loss: 0.06714714\n",
            "Epoch: 481 Loss: 0.06659233\n",
            "Epoch: 482 Loss: 0.0659838\n",
            "Epoch: 483 Loss: 0.06540795\n",
            "Epoch: 484 Loss: 0.06496163\n",
            "Epoch: 485 Loss: 0.06428007\n",
            "Epoch: 486 Loss: 0.06366045\n",
            "Epoch: 487 Loss: 0.063304685\n",
            "Epoch: 488 Loss: 0.06281748\n",
            "Epoch: 489 Loss: 0.06226184\n",
            "Epoch: 490 Loss: 0.061647996\n",
            "Epoch: 491 Loss: 0.061068024\n",
            "Epoch: 492 Loss: 0.060552794\n",
            "Epoch: 493 Loss: 0.059939392\n",
            "Epoch: 494 Loss: 0.0595674\n",
            "Epoch: 495 Loss: 0.058976978\n",
            "Epoch: 496 Loss: 0.05832791\n",
            "Epoch: 497 Loss: 0.05804937\n",
            "Epoch: 498 Loss: 0.057554632\n",
            "Epoch: 499 Loss: 0.05698307\n",
            "Epoch: 500 Loss: 0.056502808\n",
            "Epoch: 501 Loss: 0.055843905\n",
            "Epoch: 502 Loss: 0.05536994\n",
            "Epoch: 503 Loss: 0.05482725\n",
            "Epoch: 504 Loss: 0.05435454\n",
            "Epoch: 505 Loss: 0.053907584\n",
            "Epoch: 506 Loss: 0.05331694\n",
            "Epoch: 507 Loss: 0.05288808\n",
            "Epoch: 508 Loss: 0.052418444\n",
            "Epoch: 509 Loss: 0.05178769\n",
            "Epoch: 510 Loss: 0.051473476\n",
            "Epoch: 511 Loss: 0.05092885\n",
            "Epoch: 512 Loss: 0.050371017\n",
            "Epoch: 513 Loss: 0.049938165\n",
            "Epoch: 514 Loss: 0.049595643\n",
            "Epoch: 515 Loss: 0.049124155\n",
            "Epoch: 516 Loss: 0.0486111\n",
            "Epoch: 517 Loss: 0.048022386\n",
            "Epoch: 518 Loss: 0.047606856\n",
            "Epoch: 519 Loss: 0.04707819\n",
            "Epoch: 520 Loss: 0.046682797\n",
            "Epoch: 521 Loss: 0.046218745\n",
            "Epoch: 522 Loss: 0.045625374\n",
            "Epoch: 523 Loss: 0.045381084\n",
            "Epoch: 524 Loss: 0.04480415\n",
            "Epoch: 525 Loss: 0.044388033\n",
            "Epoch: 526 Loss: 0.044011828\n",
            "Epoch: 527 Loss: 0.04342789\n",
            "Epoch: 528 Loss: 0.043074068\n",
            "Epoch: 529 Loss: 0.04263927\n",
            "Epoch: 530 Loss: 0.042160127\n",
            "Epoch: 531 Loss: 0.04177704\n",
            "Epoch: 532 Loss: 0.041341927\n",
            "Epoch: 533 Loss: 0.040887184\n",
            "Epoch: 534 Loss: 0.040506303\n",
            "Epoch: 535 Loss: 0.040100284\n",
            "Epoch: 536 Loss: 0.03960469\n",
            "Epoch: 537 Loss: 0.03926515\n",
            "Epoch: 538 Loss: 0.038799185\n",
            "Epoch: 539 Loss: 0.038425583\n",
            "Epoch: 540 Loss: 0.03808813\n",
            "Epoch: 541 Loss: 0.037719667\n",
            "Epoch: 542 Loss: 0.037346855\n",
            "Epoch: 543 Loss: 0.036826566\n",
            "Epoch: 544 Loss: 0.036438216\n",
            "Epoch: 545 Loss: 0.036000777\n",
            "Epoch: 546 Loss: 0.035704043\n",
            "Epoch: 547 Loss: 0.03522574\n",
            "Epoch: 548 Loss: 0.03499621\n",
            "Epoch: 549 Loss: 0.034611158\n",
            "Epoch: 550 Loss: 0.034256794\n",
            "Epoch: 551 Loss: 0.0338071\n",
            "Epoch: 552 Loss: 0.033377185\n",
            "Epoch: 553 Loss: 0.033057537\n",
            "Epoch: 554 Loss: 0.03281668\n",
            "Epoch: 555 Loss: 0.032391287\n",
            "Epoch: 556 Loss: 0.03202177\n",
            "Epoch: 557 Loss: 0.031623054\n",
            "Epoch: 558 Loss: 0.03126272\n",
            "Epoch: 559 Loss: 0.030976001\n",
            "Epoch: 560 Loss: 0.030529471\n",
            "Epoch: 561 Loss: 0.03032429\n",
            "Epoch: 562 Loss: 0.030014135\n",
            "Epoch: 563 Loss: 0.02964281\n",
            "Epoch: 564 Loss: 0.029224884\n",
            "Epoch: 565 Loss: 0.028958745\n",
            "Epoch: 566 Loss: 0.02859459\n",
            "Epoch: 567 Loss: 0.028259607\n",
            "Epoch: 568 Loss: 0.027966198\n",
            "Epoch: 569 Loss: 0.027756898\n",
            "Epoch: 570 Loss: 0.027364824\n",
            "Epoch: 571 Loss: 0.027059486\n",
            "Epoch: 572 Loss: 0.026694132\n",
            "Epoch: 573 Loss: 0.026427813\n",
            "Epoch: 574 Loss: 0.026089977\n",
            "Epoch: 575 Loss: 0.025826562\n",
            "Epoch: 576 Loss: 0.025487563\n",
            "Epoch: 577 Loss: 0.025243685\n",
            "Epoch: 578 Loss: 0.025039723\n",
            "Epoch: 579 Loss: 0.024682088\n",
            "Epoch: 580 Loss: 0.024398435\n",
            "Epoch: 581 Loss: 0.024050301\n",
            "Epoch: 582 Loss: 0.0238174\n",
            "Epoch: 583 Loss: 0.023493651\n",
            "Epoch: 584 Loss: 0.023320293\n",
            "Epoch: 585 Loss: 0.023029985\n",
            "Epoch: 586 Loss: 0.022786971\n",
            "Epoch: 587 Loss: 0.022419551\n",
            "Epoch: 588 Loss: 0.022225272\n",
            "Epoch: 589 Loss: 0.021892827\n",
            "Epoch: 590 Loss: 0.021702154\n",
            "Epoch: 591 Loss: 0.02139694\n",
            "Epoch: 592 Loss: 0.021157343\n",
            "Epoch: 593 Loss: 0.021027487\n",
            "Epoch: 594 Loss: 0.020725071\n",
            "Epoch: 595 Loss: 0.020436525\n",
            "Epoch: 596 Loss: 0.02016778\n",
            "Epoch: 597 Loss: 0.019978894\n",
            "Epoch: 598 Loss: 0.019653505\n",
            "Epoch: 599 Loss: 0.01950968\n",
            "Epoch: 600 Loss: 0.019231608\n",
            "Epoch: 601 Loss: 0.019138686\n",
            "Epoch: 602 Loss: 0.018825283\n",
            "Epoch: 603 Loss: 0.018594608\n",
            "Epoch: 604 Loss: 0.018343562\n",
            "Epoch: 605 Loss: 0.018114202\n",
            "Epoch: 606 Loss: 0.017916739\n",
            "Epoch: 607 Loss: 0.017766772\n",
            "Epoch: 608 Loss: 0.017564565\n",
            "Epoch: 609 Loss: 0.017257493\n",
            "Epoch: 610 Loss: 0.017095674\n",
            "Epoch: 611 Loss: 0.016866399\n",
            "Epoch: 612 Loss: 0.016650172\n",
            "Epoch: 613 Loss: 0.016462624\n",
            "Epoch: 614 Loss: 0.01629154\n",
            "Epoch: 615 Loss: 0.016171332\n",
            "Epoch: 616 Loss: 0.015884671\n",
            "Epoch: 617 Loss: 0.015687902\n",
            "Epoch: 618 Loss: 0.015513418\n",
            "Epoch: 619 Loss: 0.015276631\n",
            "Epoch: 620 Loss: 0.015180459\n",
            "Epoch: 621 Loss: 0.014978736\n",
            "Epoch: 622 Loss: 0.014804224\n",
            "Epoch: 623 Loss: 0.014577581\n",
            "Epoch: 624 Loss: 0.014396198\n",
            "Epoch: 625 Loss: 0.014250249\n",
            "Epoch: 626 Loss: 0.014021168\n",
            "Epoch: 627 Loss: 0.0139435\n",
            "Epoch: 628 Loss: 0.013751915\n",
            "Epoch: 629 Loss: 0.013582325\n",
            "Epoch: 630 Loss: 0.013398243\n",
            "Epoch: 631 Loss: 0.013201495\n",
            "Epoch: 632 Loss: 0.013080187\n",
            "Epoch: 633 Loss: 0.012879124\n",
            "Epoch: 634 Loss: 0.012757346\n",
            "Epoch: 635 Loss: 0.012606866\n",
            "Epoch: 636 Loss: 0.01247404\n",
            "Epoch: 637 Loss: 0.012330721\n",
            "Epoch: 638 Loss: 0.012116052\n",
            "Epoch: 639 Loss: 0.012000784\n",
            "Epoch: 640 Loss: 0.011831667\n",
            "Epoch: 641 Loss: 0.011661463\n",
            "Epoch: 642 Loss: 0.011619171\n",
            "Epoch: 643 Loss: 0.011440563\n",
            "Epoch: 644 Loss: 0.011281978\n",
            "Epoch: 645 Loss: 0.011132475\n",
            "Epoch: 646 Loss: 0.010966849\n",
            "Epoch: 647 Loss: 0.010894554\n",
            "Epoch: 648 Loss: 0.010759588\n",
            "Epoch: 649 Loss: 0.010599798\n",
            "Epoch: 650 Loss: 0.010481604\n",
            "Epoch: 651 Loss: 0.010319134\n",
            "Epoch: 652 Loss: 0.010225677\n",
            "Epoch: 653 Loss: 0.010079489\n",
            "Epoch: 654 Loss: 0.009930601\n",
            "Epoch: 655 Loss: 0.009845235\n",
            "Epoch: 656 Loss: 0.009737171\n",
            "Epoch: 657 Loss: 0.009636336\n",
            "Epoch: 658 Loss: 0.009488757\n",
            "Epoch: 659 Loss: 0.009335194\n",
            "Epoch: 660 Loss: 0.009254312\n",
            "Epoch: 661 Loss: 0.009124741\n",
            "Epoch: 662 Loss: 0.009016213\n",
            "Epoch: 663 Loss: 0.008958644\n",
            "Epoch: 664 Loss: 0.008791898\n",
            "Epoch: 665 Loss: 0.0086906655\n",
            "Epoch: 666 Loss: 0.008582683\n",
            "Epoch: 667 Loss: 0.008451602\n",
            "Epoch: 668 Loss: 0.008380799\n",
            "Epoch: 669 Loss: 0.008299744\n",
            "Epoch: 670 Loss: 0.008152986\n",
            "Epoch: 671 Loss: 0.008083898\n",
            "Epoch: 672 Loss: 0.007954865\n",
            "Epoch: 673 Loss: 0.007851029\n",
            "Epoch: 674 Loss: 0.007771098\n",
            "Epoch: 675 Loss: 0.0076501654\n",
            "Epoch: 676 Loss: 0.007610602\n",
            "Epoch: 677 Loss: 0.007492704\n",
            "Epoch: 678 Loss: 0.007374942\n",
            "Epoch: 679 Loss: 0.007304416\n",
            "Epoch: 680 Loss: 0.007198429\n",
            "Epoch: 681 Loss: 0.007088904\n",
            "Epoch: 682 Loss: 0.007044809\n",
            "Epoch: 683 Loss: 0.006961765\n",
            "Epoch: 684 Loss: 0.0068458044\n",
            "Epoch: 685 Loss: 0.006781491\n",
            "Epoch: 686 Loss: 0.006674363\n",
            "Epoch: 687 Loss: 0.006590515\n",
            "Epoch: 688 Loss: 0.006517014\n",
            "Epoch: 689 Loss: 0.0064177928\n",
            "Epoch: 690 Loss: 0.00634843\n",
            "Epoch: 691 Loss: 0.006311523\n",
            "Epoch: 692 Loss: 0.0062053297\n",
            "Epoch: 693 Loss: 0.006122561\n",
            "Epoch: 694 Loss: 0.006043828\n",
            "Epoch: 695 Loss: 0.0059491694\n",
            "Epoch: 696 Loss: 0.005899313\n",
            "Epoch: 697 Loss: 0.0058512753\n",
            "Epoch: 698 Loss: 0.0057402668\n",
            "Epoch: 699 Loss: 0.005687893\n",
            "Epoch: 700 Loss: 0.005603622\n",
            "Epoch: 701 Loss: 0.005515707\n",
            "Epoch: 702 Loss: 0.0054698847\n",
            "Epoch: 703 Loss: 0.0053893863\n",
            "Epoch: 704 Loss: 0.005311288\n",
            "Epoch: 705 Loss: 0.00530651\n",
            "Epoch: 706 Loss: 0.0052045747\n",
            "Epoch: 707 Loss: 0.0051202904\n",
            "Epoch: 708 Loss: 0.0050740484\n",
            "Epoch: 709 Loss: 0.004994051\n",
            "Epoch: 710 Loss: 0.004925452\n",
            "Epoch: 711 Loss: 0.0048848838\n",
            "Epoch: 712 Loss: 0.0048399465\n",
            "Epoch: 713 Loss: 0.0047613746\n",
            "Epoch: 714 Loss: 0.0047047567\n",
            "Epoch: 715 Loss: 0.0046311235\n",
            "Epoch: 716 Loss: 0.004570625\n",
            "Epoch: 717 Loss: 0.0045248903\n",
            "Epoch: 718 Loss: 0.004486026\n",
            "Epoch: 719 Loss: 0.004411921\n",
            "Epoch: 720 Loss: 0.0043618414\n",
            "Epoch: 721 Loss: 0.0042935642\n",
            "Epoch: 722 Loss: 0.0042369985\n",
            "Epoch: 723 Loss: 0.004196429\n",
            "Epoch: 724 Loss: 0.0041402737\n",
            "Epoch: 725 Loss: 0.004100697\n",
            "Epoch: 726 Loss: 0.004047383\n",
            "Epoch: 727 Loss: 0.003982555\n",
            "Epoch: 728 Loss: 0.0039272723\n",
            "Epoch: 729 Loss: 0.0038910317\n",
            "Epoch: 730 Loss: 0.0038358928\n",
            "Epoch: 731 Loss: 0.003794704\n",
            "Epoch: 732 Loss: 0.003752621\n",
            "Epoch: 733 Loss: 0.003693182\n",
            "Epoch: 734 Loss: 0.0036381413\n",
            "Epoch: 735 Loss: 0.0036087837\n",
            "Epoch: 736 Loss: 0.0035522534\n",
            "Epoch: 737 Loss: 0.0034988734\n",
            "Epoch: 738 Loss: 0.00349383\n",
            "Epoch: 739 Loss: 0.003426212\n",
            "Epoch: 740 Loss: 0.003372583\n",
            "Epoch: 741 Loss: 0.0033449766\n",
            "Epoch: 742 Loss: 0.0032953657\n",
            "Epoch: 743 Loss: 0.0032436848\n",
            "Epoch: 744 Loss: 0.0032124673\n",
            "Epoch: 745 Loss: 0.003178888\n",
            "Epoch: 746 Loss: 0.003140906\n",
            "Epoch: 747 Loss: 0.0030950361\n",
            "Epoch: 748 Loss: 0.0030574005\n",
            "Epoch: 749 Loss: 0.0030088918\n",
            "Epoch: 750 Loss: 0.0029728205\n",
            "Epoch: 751 Loss: 0.0029422073\n",
            "Epoch: 752 Loss: 0.0029129912\n",
            "Epoch: 753 Loss: 0.0028643508\n",
            "Epoch: 754 Loss: 0.0028358386\n",
            "Epoch: 755 Loss: 0.0027906697\n",
            "Epoch: 756 Loss: 0.0027508838\n",
            "Epoch: 757 Loss: 0.002726807\n",
            "Epoch: 758 Loss: 0.002692431\n",
            "Epoch: 759 Loss: 0.0026578028\n",
            "Epoch: 760 Loss: 0.0026294268\n",
            "Epoch: 761 Loss: 0.0025898498\n",
            "Epoch: 762 Loss: 0.0025490357\n",
            "Epoch: 763 Loss: 0.0025229724\n",
            "Epoch: 764 Loss: 0.0024925112\n",
            "Epoch: 765 Loss: 0.0024657082\n",
            "Epoch: 766 Loss: 0.00242934\n",
            "Epoch: 767 Loss: 0.0024028416\n",
            "Epoch: 768 Loss: 0.0023645833\n",
            "Epoch: 769 Loss: 0.002331121\n",
            "Epoch: 770 Loss: 0.0023101678\n",
            "Epoch: 771 Loss: 0.0022823003\n",
            "Epoch: 772 Loss: 0.0022483533\n",
            "Epoch: 773 Loss: 0.0022274954\n",
            "Epoch: 774 Loss: 0.0021938847\n",
            "Epoch: 775 Loss: 0.0021591696\n",
            "Epoch: 776 Loss: 0.0021360405\n",
            "Epoch: 777 Loss: 0.0021145768\n",
            "Epoch: 778 Loss: 0.002085501\n",
            "Epoch: 779 Loss: 0.002057373\n",
            "Epoch: 780 Loss: 0.0020363072\n",
            "Epoch: 781 Loss: 0.0020034027\n",
            "Epoch: 782 Loss: 0.0019733522\n",
            "Epoch: 783 Loss: 0.0019569704\n",
            "Epoch: 784 Loss: 0.0019320641\n",
            "Epoch: 785 Loss: 0.001904109\n",
            "Epoch: 786 Loss: 0.001886224\n",
            "Epoch: 787 Loss: 0.0018593465\n",
            "Epoch: 788 Loss: 0.0018294373\n",
            "Epoch: 789 Loss: 0.0018080937\n",
            "Epoch: 790 Loss: 0.0017870468\n",
            "Epoch: 791 Loss: 0.0017661101\n",
            "Epoch: 792 Loss: 0.0017408961\n",
            "Epoch: 793 Loss: 0.0017256798\n",
            "Epoch: 794 Loss: 0.0016973711\n",
            "Epoch: 795 Loss: 0.0016703063\n",
            "Epoch: 796 Loss: 0.0016560603\n",
            "Epoch: 797 Loss: 0.0016336931\n",
            "Epoch: 798 Loss: 0.0016136637\n",
            "Epoch: 799 Loss: 0.0015939537\n",
            "Epoch: 800 Loss: 0.001575276\n",
            "Epoch: 801 Loss: 0.0015496927\n",
            "Epoch: 802 Loss: 0.0015271864\n",
            "Epoch: 803 Loss: 0.0015135838\n",
            "Epoch: 804 Loss: 0.0014953262\n",
            "Epoch: 805 Loss: 0.0014729775\n",
            "Epoch: 806 Loss: 0.0014585889\n",
            "Epoch: 807 Loss: 0.001438132\n",
            "Epoch: 808 Loss: 0.0014148937\n",
            "Epoch: 809 Loss: 0.0013968142\n",
            "Epoch: 810 Loss: 0.0013838459\n",
            "Epoch: 811 Loss: 0.0013668017\n",
            "Epoch: 812 Loss: 0.0013444134\n",
            "Epoch: 813 Loss: 0.0013337905\n",
            "Epoch: 814 Loss: 0.0013128162\n",
            "Epoch: 815 Loss: 0.0012917275\n",
            "Epoch: 816 Loss: 0.001276708\n",
            "Epoch: 817 Loss: 0.0012668786\n",
            "Epoch: 818 Loss: 0.0012478754\n",
            "Epoch: 819 Loss: 0.0012282163\n",
            "Epoch: 820 Loss: 0.0012180053\n",
            "Epoch: 821 Loss: 0.0011980985\n",
            "Epoch: 822 Loss: 0.0011790084\n",
            "Epoch: 823 Loss: 0.0011698625\n",
            "Epoch: 824 Loss: 0.0011578884\n",
            "Epoch: 825 Loss: 0.0011385619\n",
            "Epoch: 826 Loss: 0.0011221762\n",
            "Epoch: 827 Loss: 0.0011115688\n",
            "Epoch: 828 Loss: 0.0010935375\n",
            "Epoch: 829 Loss: 0.0010773137\n",
            "Epoch: 830 Loss: 0.001070898\n",
            "Epoch: 831 Loss: 0.0010562931\n",
            "Epoch: 832 Loss: 0.0010389239\n",
            "Epoch: 833 Loss: 0.0010246385\n",
            "Epoch: 834 Loss: 0.0010144422\n",
            "Epoch: 835 Loss: 0.000998068\n",
            "Epoch: 836 Loss: 0.0009865881\n",
            "Epoch: 837 Loss: 0.0009773978\n",
            "Epoch: 838 Loss: 0.00096408254\n",
            "Epoch: 839 Loss: 0.0009482409\n",
            "Epoch: 840 Loss: 0.0009349999\n",
            "Epoch: 841 Loss: 0.00092589104\n",
            "Epoch: 842 Loss: 0.00091332855\n",
            "Epoch: 843 Loss: 0.00090114045\n",
            "Epoch: 844 Loss: 0.0008919186\n",
            "Epoch: 845 Loss: 0.0008795655\n",
            "Epoch: 846 Loss: 0.00086522236\n",
            "Epoch: 847 Loss: 0.0008529689\n",
            "Epoch: 848 Loss: 0.0008457488\n",
            "Epoch: 849 Loss: 0.0008357601\n",
            "Epoch: 850 Loss: 0.0008219319\n",
            "Epoch: 851 Loss: 0.0008135817\n",
            "Epoch: 852 Loss: 0.0008024221\n",
            "Epoch: 853 Loss: 0.0007894205\n",
            "Epoch: 854 Loss: 0.00077788066\n",
            "Epoch: 855 Loss: 0.00077530195\n",
            "Epoch: 856 Loss: 0.00076217426\n",
            "Epoch: 857 Loss: 0.0007497341\n",
            "Epoch: 858 Loss: 0.0007418258\n",
            "Epoch: 859 Loss: 0.0007320376\n",
            "Epoch: 860 Loss: 0.00072022807\n",
            "Epoch: 861 Loss: 0.00071178534\n",
            "Epoch: 862 Loss: 0.00070732896\n",
            "Epoch: 863 Loss: 0.00069537334\n",
            "Epoch: 864 Loss: 0.00068402954\n",
            "Epoch: 865 Loss: 0.0006760451\n",
            "Epoch: 866 Loss: 0.00066788634\n",
            "Epoch: 867 Loss: 0.0006586904\n",
            "Epoch: 868 Loss: 0.0006500794\n",
            "Epoch: 869 Loss: 0.00064430805\n",
            "Epoch: 870 Loss: 0.0006344538\n",
            "Epoch: 871 Loss: 0.0006240781\n",
            "Epoch: 872 Loss: 0.0006157135\n",
            "Epoch: 873 Loss: 0.0006098728\n",
            "Epoch: 874 Loss: 0.00060279167\n",
            "Epoch: 875 Loss: 0.0005927723\n",
            "Epoch: 876 Loss: 0.0005869232\n",
            "Epoch: 877 Loss: 0.00057862955\n",
            "Epoch: 878 Loss: 0.00056921155\n",
            "Epoch: 879 Loss: 0.00056087057\n",
            "Epoch: 880 Loss: 0.0005590308\n",
            "Epoch: 881 Loss: 0.00054951286\n",
            "Epoch: 882 Loss: 0.00054049894\n",
            "Epoch: 883 Loss: 0.0005345628\n",
            "Epoch: 884 Loss: 0.00052768044\n",
            "Epoch: 885 Loss: 0.0005191271\n",
            "Epoch: 886 Loss: 0.000513043\n",
            "Epoch: 887 Loss: 0.00050915946\n",
            "Epoch: 888 Loss: 0.0005013943\n",
            "Epoch: 889 Loss: 0.0004930877\n",
            "Epoch: 890 Loss: 0.00048644218\n",
            "Epoch: 891 Loss: 0.0004813543\n",
            "Epoch: 892 Loss: 0.00047484963\n",
            "Epoch: 893 Loss: 0.00046845182\n",
            "Epoch: 894 Loss: 0.00046352317\n",
            "Epoch: 895 Loss: 0.0004571195\n",
            "Epoch: 896 Loss: 0.00044959286\n",
            "Epoch: 897 Loss: 0.00044279228\n",
            "Epoch: 898 Loss: 0.00043963222\n",
            "Epoch: 899 Loss: 0.00043420232\n",
            "Epoch: 900 Loss: 0.00042694353\n",
            "Epoch: 901 Loss: 0.00042194882\n",
            "Epoch: 902 Loss: 0.00041671153\n",
            "Epoch: 903 Loss: 0.0004098953\n",
            "Epoch: 904 Loss: 0.00040373663\n",
            "Epoch: 905 Loss: 0.00040187288\n",
            "Epoch: 906 Loss: 0.00039585974\n",
            "Epoch: 907 Loss: 0.00038926367\n",
            "Epoch: 908 Loss: 0.00038381357\n",
            "Epoch: 909 Loss: 0.00037996023\n",
            "Epoch: 910 Loss: 0.000373778\n",
            "Epoch: 911 Loss: 0.00036976664\n",
            "Epoch: 912 Loss: 0.00036563008\n",
            "Epoch: 913 Loss: 0.000360788\n",
            "Epoch: 914 Loss: 0.00035482872\n",
            "Epoch: 915 Loss: 0.00034916046\n",
            "Epoch: 916 Loss: 0.0003463852\n",
            "Epoch: 917 Loss: 0.00034223188\n",
            "Epoch: 918 Loss: 0.0003370738\n",
            "Epoch: 919 Loss: 0.00033256965\n",
            "Epoch: 920 Loss: 0.000328907\n",
            "Epoch: 921 Loss: 0.00032347487\n",
            "Epoch: 922 Loss: 0.0003182597\n",
            "Epoch: 923 Loss: 0.00031594955\n",
            "Epoch: 924 Loss: 0.00031255916\n",
            "Epoch: 925 Loss: 0.00030724832\n",
            "Epoch: 926 Loss: 0.000302388\n",
            "Epoch: 927 Loss: 0.0002998296\n",
            "Epoch: 928 Loss: 0.00029488112\n",
            "Epoch: 929 Loss: 0.00029070288\n",
            "Epoch: 930 Loss: 0.00028799608\n",
            "Epoch: 931 Loss: 0.00028474088\n",
            "Epoch: 932 Loss: 0.0002799674\n",
            "Epoch: 933 Loss: 0.00027541092\n",
            "Epoch: 934 Loss: 0.00027278636\n",
            "Epoch: 935 Loss: 0.00026901037\n",
            "Epoch: 936 Loss: 0.00026599708\n",
            "Epoch: 937 Loss: 0.00026177865\n",
            "Epoch: 938 Loss: 0.00025947913\n",
            "Epoch: 939 Loss: 0.00025515308\n",
            "Epoch: 940 Loss: 0.00025101218\n",
            "Epoch: 941 Loss: 0.00024799278\n",
            "Epoch: 942 Loss: 0.00024651314\n",
            "Epoch: 943 Loss: 0.0002423026\n",
            "Epoch: 944 Loss: 0.00023831097\n",
            "Epoch: 945 Loss: 0.00023606945\n",
            "Epoch: 946 Loss: 0.00023261308\n",
            "Epoch: 947 Loss: 0.00022881737\n",
            "Epoch: 948 Loss: 0.0002262073\n",
            "Epoch: 949 Loss: 0.0002247095\n",
            "Epoch: 950 Loss: 0.00022085586\n",
            "Epoch: 951 Loss: 0.00021720583\n",
            "Epoch: 952 Loss: 0.00021449638\n",
            "Epoch: 953 Loss: 0.00021201244\n",
            "Epoch: 954 Loss: 0.00020918011\n",
            "Epoch: 955 Loss: 0.00020632008\n",
            "Epoch: 956 Loss: 0.00020416325\n",
            "Epoch: 957 Loss: 0.00020129394\n",
            "Epoch: 958 Loss: 0.00019795718\n",
            "Epoch: 959 Loss: 0.00019484879\n",
            "Epoch: 960 Loss: 0.00019357134\n",
            "Epoch: 961 Loss: 0.00019114686\n",
            "Epoch: 962 Loss: 0.00018792527\n",
            "Epoch: 963 Loss: 0.00018552631\n",
            "Epoch: 964 Loss: 0.00018338702\n",
            "Epoch: 965 Loss: 0.00018036373\n",
            "Epoch: 966 Loss: 0.0001776894\n",
            "Epoch: 967 Loss: 0.00017656296\n",
            "Epoch: 968 Loss: 0.00017415453\n",
            "Epoch: 969 Loss: 0.00017122834\n",
            "Epoch: 970 Loss: 0.00016849925\n",
            "Epoch: 971 Loss: 0.00016710587\n",
            "Epoch: 972 Loss: 0.00016443919\n",
            "Epoch: 973 Loss: 0.00016259952\n",
            "Epoch: 974 Loss: 0.00016040655\n",
            "Epoch: 975 Loss: 0.00015862164\n",
            "Epoch: 976 Loss: 0.00015598079\n",
            "Epoch: 977 Loss: 0.00015344967\n",
            "Epoch: 978 Loss: 0.00015183406\n",
            "Epoch: 979 Loss: 0.00015045558\n",
            "Epoch: 980 Loss: 0.00014818918\n",
            "Epoch: 981 Loss: 0.00014571211\n",
            "Epoch: 982 Loss: 0.00014445072\n",
            "Epoch: 983 Loss: 0.00014219707\n",
            "Epoch: 984 Loss: 0.00013985822\n",
            "Epoch: 985 Loss: 0.00013825289\n",
            "Epoch: 986 Loss: 0.00013734754\n",
            "Epoch: 987 Loss: 0.00013497511\n",
            "Epoch: 988 Loss: 0.00013273262\n",
            "Epoch: 989 Loss: 0.00013117552\n",
            "Epoch: 990 Loss: 0.00012954342\n",
            "Epoch: 991 Loss: 0.00012774442\n",
            "Epoch: 992 Loss: 0.00012605856\n",
            "Epoch: 993 Loss: 0.00012481306\n",
            "Epoch: 994 Loss: 0.00012297356\n",
            "Epoch: 995 Loss: 0.00012092768\n",
            "Epoch: 996 Loss: 0.00011907422\n",
            "Epoch: 997 Loss: 0.00011820441\n",
            "Epoch: 998 Loss: 0.00011675824\n",
            "Epoch: 999 Loss: 0.000114782604\n",
            "Epoch: 1000 Loss: 0.00011334089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LOmonwRSVqui",
        "colab_type": "code",
        "outputId": "99804648-55c1-46b6-b4fc-0c3116fd1b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(round(model.inferring(from: [[0, 0], [0, 1], [1, 0], [1, 1]])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0], [1.0], [1.0], [0.0]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mbVuhOwdnjXf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}